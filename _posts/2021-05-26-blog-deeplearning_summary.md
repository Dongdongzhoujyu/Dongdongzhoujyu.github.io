---
title: '什么是deep learning'
date: 2021-05-26
permalink: /posts/2021/05/2021-05-26-blog-deeplearning_summary/
tags:
  - deeplearning
  - 随笔
---

前言
=====
现在是2021年5月，工作了一年，回忆一下吧，记录一下此刻自己对深度学习的一些认识，希望以后回首，能看到年轻的自己naive的想法。

自己应该是很久没有说话，也很久没有总结了，可能现在待的这个小公司有些思想固化，感觉总没有一套方法论的东西，遇到问题思考的调理也不清晰，如果有人问，怎么理解深度学习，竟一时语塞，只在说深度学习就是一个拟合工具，通过定义损失函数去逼近需要的功能，然后，没了！这里反思一下，为什么会这样，重新思考，如果让你去介绍深度学习，你应该怎么介绍，或者让你去介绍人工智能，应该怎么介绍。

正文
=====
首先应该抓住一些要阐明的关键点，给出深度学习是如果定义的，深度学习包含那些要素，深度学习可以解决哪些方面的问题，深度学习对应的都有哪些应用，现阶段它都存在哪些问题。下文就据此框架展开，在必要的地方会有进一步的说明。

深度学习是一个基于特征学习的机器学习方法，可以定义为一个黑箱，通过MLP或者CNN来拟合一个函数，进行域到域的转换，比如从图片域到标签域，将输入转化成我们需要对应的结果，理想状态，它是一个万能的拟合函数。深度学习是数据驱动的，如果设定好目标函数，那个喂给它什么数据，基本上它就能学习到什么样的知识，比如识别猫狗的网络，基本就只能来干这一件事，当然也有一些任务，可以拓展到识别未知的类别，比如zero-shot相关或则few-shot相关的任务。基本上，只要我们能找到输入和输出的映射关系，我们就可以用深度学习来解决对应的问题。

它由一些基础的组件构成，是连接主义盛行的体现，包括基本的全连接层，卷积层，池化层，激活层，还有与之相关的优化器等等，包含LSTM，GRU等等，这其中每一个部分又包含来许多小的内容，比如
- 各个层的计算量，参数量如何定义
- 全连接层和1\*1的卷积对应的关系，
- 各种各样的卷积，包括：1\*1的卷积，空洞卷积，反卷积，深度可分离卷积，分组卷积等等，
- 与之对应的各种经典的网络结构，包括skip-connect，dense-net，inception-net系列等等，
- 各种池化操作，包括最大最小，以及全局池化，组合池化等等
- 各种激活函数，包括relu，swish，softmax等等
- 各种优化器，包括SGD，Adam，MomentSGD，NesterovSGD，AdaGrad，RMSProp等等
- 各种损失函数，mse，crossentropy，GAN loss，abs等，其中绝大所属都是自监督的损失或者和熵相关的损失
- 其他基础的组件，包括BN，Dropout等等
这些领域，每一个都可以展开进行许多工作，待有精力再将某一部分细说。

深度学习可以解决很多机器学习领域的问题，包括监督学习，半监督学习和无监督学习。

监督学习比较好理解，比如对于分类，定义好样本标签，通过交叉熵就可以来使模型不断学习图片对应的标签信息，又比如对于图像超分问题，定义好highresolution的图片和lowresolution的图片，直接通过MSE的损失就可以使网络学习从高分辨率到低分辨率的退化过程，当然很多时候这样直接使用标签信息不能很好地完成相应的任务，很多时候我们需要有一些先验知识或者添加一些正则化的手段，来使模型学习到更好的知识。基本上多数任务都可以定义为监督学习的领域，比如图像分类，图像超分，图像去模糊，目标检测，实例分割等，深度学习通常这些任务也都有远优于传统方法的表现。

半监督学习通常只有很少一部分带标签数据，通过设计算法，需要在大量无标签的数据中进行特征引导或者提取，解决相应的问题，目前自己接触到的半监督学习的任务基本都是围绕分类展开的。其实在样本数很少的时候还对应这few-shot相关的任务，不过两者的侧重点不太相同而已，few-shot的主要任务是在接触到很少的样本后，能够对不同样本，甚至未知的样本进行区分，它更多强调的是一种学习能力，所以很多点和meta-learning相关。

当前semi-supervised learning 的套路最多的是通过对数据进行增强，学习一个consistent的损失，或者对数据和标签进行融合，比如maxup，然后学习融合后的数据分布，也有一些方法通过定一个memory的结构，通过在学习中不断更新一个中心，来学习无标签数据中通过带标签数据引导的有利于分类的信息，也有些方法通过定义样本相似性，得到伪标签来进行后续的分类任务，多数的套路都和active learning类似，通过得到伪标签以期望模型有一个正向的循环反馈。

few-shot则是训练模型学习的能力，我了解到的是通过度量学习的方式来定义正负样本之间的损失，让模型学习分辨正负样本的能力，现阶段few-shot仍只对应一些规模比较小的任务。

无监督对应到深度学习，主要可以分为两个大的方向，一个是基于GAN和VAE的生成模型，另一个则是基于强化学习的深度强化学习，对于第二个领域，目前的认知是，强化学习训练的效率极低，但是可以在仿真环境中进行无限次的训练，这是目前通向强人工智能的一个方向，因为最后AI的形态肯定是人类无法理解的，深度强化学习目前包括的基础算法有DQN，SASAS，DDPG，A3C等等，其实自己对这个领域也只是窥探了皮毛。

GAN可以说是深度学习领域最著名的产物之一，通过生成器和鉴别器之间的博弈，可以进行很多有趣的事情，比如生成不同风格的图片，在不同的图片域之间进行转换，换脸，给人脸添加不同的属性，辅助图像处理，比如进行超分，提升low level任务的性能。GAN通过最大化鉴别器的损失和最小化生成器的损失，通过交替优化，来得到一个更好的生成器，使之达到以假乱真的作用，GAN的损失定义如下：
- $min_{G}max_{D} ~~ E_{x\sim p(x)}log(D(x)) + E_{z\sim q(z)}log(1-D(G(z)))$
- 最小化生成器，此时只关注第二部分，损失函数希望最大化$D(G(z))$，也就是说希望生成器得到的东西鉴别器鉴别为1，即希望生成更真的东西
- 最大化鉴别器，此时两部分都要关注，即希望第一部分尽可能大，对应希望将真实样本鉴别为1，最大化第二部分也就是最小化$D(G(z))$，即希望鉴别器将采样生成的样本都鉴别为0

GAN最大的问题就是在于难训练，所以学者们定义来许多其他的loss，比如WGAN，LSGAN等等。
另外在GAN中，个人感觉最有趣的就是CycleGAN，通过定义两个G和两个D来沟通两个不同的数据域，可以完成很多无监督的任务，比如图像转换，以及图像超分等等。

VAE是一种生成模型，直观上它更像一个autoencoder，但是它定义来每个样本的概率分布，然后通过在样本对应的概率分布中进行采样，重新生成样本，也就是说它原理上来说其实不是重构，VAE通过定义和原始数据的MSE损失和拟合出的均值和方差和标准正态分布的KL散度来优化整个模型，直觉上，刚开始的时候肯定MSE损失占比较大，此时模型倾向于生成更真的样本，而随着训练的进行，KL散度占的比例会加大，此时模型会倾向于生成更多样化的样本，两者不断博弈，也有一丝生成对抗的意味。

以上介绍来自己现阶段对深度学习一些方面的认识，当然AI领域对应很多相关的应用，比如，目标检测，跟踪，行人重识别，人体骨架检测，去雾去雨，点云分类、分割，图像复原，模型量化压缩剪枝，以及nlp相关的领域，包括翻译，文本分类，等等很多领域都是自己未涉及的，前路漫漫，何处是归途？

深度学习有它的优势，在一些领域几乎吊打传统方法，但是它也有很多问题，比如工业界普遍头疼的CV难落地。首先，它在对精度要求极高的一些领域，比如立体视觉、三维重建等领域，性能远远比不上基于数学的方法；第二，深度学习模型普遍存在一个问题，就是只能针对特定的领域，基本上喂什么数据，才能学习到什么知识，难具泛化性；第三，在一些对原理性要求极高的领域，深度学习往往像一个黑匣子，可解释性太差；第四，抗干扰性极差，一些进行模型攻击的方法，使用很简单的一些trick就能是模型完全失效，比如对于梯度的攻击。

现阶段，深度学习不是万能的，它仍需结合许多先验知识才能更好的完成想要的任务，但其确实在很多领域已经吊打传统方法。


### attention transform bert， 神经网络初始化，pytorch和tensorflow的区别




